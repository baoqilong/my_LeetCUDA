Implement Linear Attention for a given set of matrices, following the method described in "Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention" . Given the query matrix Q of size M×d, key matrix K of size M×d, and value matrix V of size M×d, your program should compute the output matrix using the formula:
 
 $\text{LinearAttention}(Q, K, V) = \frac{\phi(Q) \left(\phi(K)^T V \right)}{\phi(Q) \left(\sum_j \phi(K_j) \right)}$
 
where $\phi(x)$ is a feature map applied element-wise, for example:
 
$ \phi(x) = \text{ELU}(x) + 1 =
  \begin{cases}
  x + 1, & x > 0 \\
  e^x, & x \le 0
  \end{cases}$
 
All matrices Q, K, V, and output are of type float32, and M and d are of type int32.

Implementation Requirements
Use only native features (external libraries are not permitted)
The solve function signature must remain unchanged
The final result must be stored in the output matrix output